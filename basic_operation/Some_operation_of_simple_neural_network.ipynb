{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从一个典型的BP神经网络介绍TensorFlow中的各种操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 下面是读取数据和初始化的基本操作，不用介绍\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 设置日志级别只是ERROR及以上，避免输出一大堆WARNING\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学习参数和网络参数\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "display_step = 100\n",
    "num_steps = 3000\n",
    "\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "num_input = 28 *28\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=[None, num_input])\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Weights = {\n",
    "    'hidden_1':tf.Variable(tf.random_normal(shape=[num_input, n_hidden_1])),\n",
    "    'hidden_2':tf.Variable(tf.random_normal(shape=[n_hidden_1, n_hidden_2])),\n",
    "    'out':tf.Variable(tf.random_normal(shape=[n_hidden_2, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden_1':tf.Variable(tf.random_normal(shape=[n_hidden_1])),\n",
    "    'hidden_2':tf.Variable(tf.random_normal(shape=[n_hidden_2])),\n",
    "    'out':tf.Variable(tf.random_normal(shape=[num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net(x):\n",
    "    layer1 = tf.add(tf.matmul(x, Weights['hidden_1']), biases['hidden_1'])\n",
    "    layer2 = tf.add(tf.matmul(layer1, Weights['hidden_2']), biases['hidden_2'])\n",
    "    out_layer = tf.add(tf.matmul(layer2, Weights['out']), biases['out'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义训练\n",
    "logits = neural_net(X)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 定义测试准确率和损失\n",
    "corrected_pred = tf.equal(tf.argmax(input=logits, axis=1), tf.argmax(input=Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(corrected_pred, tf.float32))\n",
    "\n",
    "# 初始化全局变量\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch loss is 3969.6597, Accuracy is 0.078\n",
      "Step 100, Minibatch loss is 367.5314, Accuracy is 0.715\n",
      "Step 200, Minibatch loss is 205.7923, Accuracy is 0.809\n",
      "Step 300, Minibatch loss is 115.6662, Accuracy is 0.840\n",
      "Step 400, Minibatch loss is 134.3223, Accuracy is 0.848\n",
      "Step 500, Minibatch loss is 91.2530, Accuracy is 0.871\n",
      "Step 600, Minibatch loss is 109.9074, Accuracy is 0.859\n",
      "Step 700, Minibatch loss is 96.4995, Accuracy is 0.863\n",
      "Step 800, Minibatch loss is 91.4111, Accuracy is 0.867\n",
      "Step 900, Minibatch loss is 108.7307, Accuracy is 0.824\n",
      "Step 1000, Minibatch loss is 62.4677, Accuracy is 0.887\n",
      "Step 1100, Minibatch loss is 74.5396, Accuracy is 0.879\n",
      "Step 1200, Minibatch loss is 62.9036, Accuracy is 0.891\n",
      "Step 1300, Minibatch loss is 51.7317, Accuracy is 0.910\n",
      "Step 1400, Minibatch loss is 71.3864, Accuracy is 0.898\n",
      "Step 1500, Minibatch loss is 31.2562, Accuracy is 0.902\n",
      "Step 1600, Minibatch loss is 43.5522, Accuracy is 0.867\n",
      "Step 1700, Minibatch loss is 48.2615, Accuracy is 0.863\n",
      "Step 1800, Minibatch loss is 50.4740, Accuracy is 0.852\n",
      "Step 1900, Minibatch loss is 32.1606, Accuracy is 0.883\n",
      "Step 2000, Minibatch loss is 27.4624, Accuracy is 0.918\n",
      "Step 2100, Minibatch loss is 42.2959, Accuracy is 0.859\n",
      "Step 2200, Minibatch loss is 27.1532, Accuracy is 0.895\n",
      "Step 2300, Minibatch loss is 33.4819, Accuracy is 0.891\n",
      "Step 2400, Minibatch loss is 32.6332, Accuracy is 0.906\n",
      "Step 2500, Minibatch loss is 17.9305, Accuracy is 0.926\n",
      "Step 2600, Minibatch loss is 13.1359, Accuracy is 0.930\n",
      "Step 2700, Minibatch loss is 42.1030, Accuracy is 0.887\n",
      "Step 2800, Minibatch loss is 14.6027, Accuracy is 0.934\n",
      "Step 2900, Minibatch loss is 26.2430, Accuracy is 0.887\n",
      "Step 3000, Minibatch loss is 16.0481, Accuracy is 0.922\n",
      "Optimize Finished!\n",
      "Last accuracy is 0.8840\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for step in range(1, num_steps + 1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "        \n",
    "        if step % display_step == 0 or step == 1:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X:batch_x, Y:batch_y})\n",
    "            \n",
    "            print(\"Step {}, Minibatch loss is {:.4f}, Accuracy is {:.3f}\".format(step, loss, acc))\n",
    "            \n",
    "    print(\"Optimize Finished!\")\n",
    "    \n",
    "    acc = sess.run(accuracy, feed_dict={X:mnist.test.images[:1000], Y: mnist.test.labels[:1000]})\n",
    "    print(\"Last accuracy is {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "这里好像除了优化的步骤外，其他的没有需要介绍。\n",
    "1. 字典的定义完全是Python的数据类型\n",
    "2. 其他的矩阵和向量的运算都是tf中的常规运算\n",
    "\n",
    "tf.nn.softmax_cross_entropy_with_logits是个什么玩意儿，好像是用来计算交叉熵软回归API。关于它的介绍在[官网链接](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "关于Softmax Regression的介绍在[UFLDL](http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92)\n",
    "\n",
    "tf.reduce_mean是降低维度，并求均值，没有指定维度信息，默认是求解所有元素的均值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
